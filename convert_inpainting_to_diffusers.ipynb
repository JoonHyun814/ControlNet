{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    BertTokenizerFast,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    ")\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDIMScheduler,\n",
    "    DDPMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    HeunDiscreteScheduler,\n",
    "    LDMTextToImagePipeline,\n",
    "    LMSDiscreteScheduler,\n",
    "    PNDMScheduler,\n",
    "    PriorTransformer,\n",
    "    StableDiffusionInpaintPipeline,\n",
    "    StableUnCLIPImg2ImgPipeline,\n",
    "    StableUnCLIPPipeline,\n",
    "    UnCLIPScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.pipelines.paint_by_example import PaintByExamplePipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import (\n",
    "    create_unet_diffusers_config, \n",
    "    convert_ldm_unet_checkpoint, \n",
    "    create_vae_diffusers_config, \n",
    "    convert_ldm_vae_checkpoint,\n",
    "    stable_unclip_image_noising_components,\n",
    "    convert_open_clip_checkpoint,\n",
    "    stable_unclip_image_encoder,\n",
    "    convert_paint_by_example_checkpoint,\n",
    "    convert_ldm_clip_checkpoint,\n",
    "    create_ldm_bert_config,\n",
    "    convert_ldm_bert_checkpoint,\n",
    "    logger\n",
    "    )\n",
    "\n",
    "def load_pipeline_inpainting_stable_diffusion_ckpt(\n",
    "    checkpoint_path: str,\n",
    "    original_config_file: str = None,\n",
    "    image_size: int = 512,\n",
    "    prediction_type: str = None,\n",
    "    model_type: str = None,\n",
    "    extract_ema: bool = False,\n",
    "    scheduler_type: str = \"pndm\",\n",
    "    num_in_channels: Optional[int] = None,\n",
    "    upcast_attention: Optional[bool] = None,\n",
    "    device: str = None,\n",
    "    from_safetensors: bool = False,\n",
    "    stable_unclip: Optional[str] = None,\n",
    "    stable_unclip_prior: Optional[str] = None,\n",
    "    clip_stats_path: Optional[str] = None,\n",
    ") -> StableDiffusionInpaintPipeline:\n",
    "    \"\"\"\n",
    "    Load a Stable Diffusion pipeline object from a CompVis-style `.ckpt`/`.safetensors` file and (ideally) a `.yaml`\n",
    "    config file.\n",
    "\n",
    "    Although many of the arguments can be automatically inferred, some of these rely on brittle checks against the\n",
    "    global step count, which will likely fail for models that have undergone further fine-tuning. Therefore, it is\n",
    "    recommended that you override the default values and/or supply an `original_config_file` wherever possible.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (`str`): Path to `.ckpt` file.\n",
    "        original_config_file (`str`):\n",
    "            Path to `.yaml` config file corresponding to the original architecture. If `None`, will be automatically\n",
    "            inferred by looking for a key that only exists in SD2.0 models.\n",
    "        image_size (`int`, *optional*, defaults to 512):\n",
    "            The image size that the model was trained on. Use 512 for Stable Diffusion v1.X and Stable Diffusion v2\n",
    "            Base. Use 768 for Stable Diffusion v2.\n",
    "        prediction_type (`str`, *optional*):\n",
    "            The prediction type that the model was trained on. Use `'epsilon'` for Stable Diffusion v1.X and Stable\n",
    "            Diffusion v2 Base. Use `'v_prediction'` for Stable Diffusion v2.\n",
    "        num_in_channels (`int`, *optional*, defaults to None):\n",
    "            The number of input channels. If `None`, it will be automatically inferred.\n",
    "        scheduler_type (`str`, *optional*, defaults to 'pndm'):\n",
    "            Type of scheduler to use. Should be one of `[\"pndm\", \"lms\", \"heun\", \"euler\", \"euler-ancestral\", \"dpm\",\n",
    "            \"ddim\"]`.\n",
    "        model_type (`str`, *optional*, defaults to `None`):\n",
    "            The pipeline type. `None` to automatically infer, or one of `[\"FrozenOpenCLIPEmbedder\",\n",
    "            \"FrozenCLIPEmbedder\", \"PaintByExample\"]`.\n",
    "        extract_ema (`bool`, *optional*, defaults to `False`): Only relevant for\n",
    "            checkpoints that have both EMA and non-EMA weights. Whether to extract the EMA weights or not. Defaults to\n",
    "            `False`. Pass `True` to extract the EMA weights. EMA weights usually yield higher quality images for\n",
    "            inference. Non-EMA weights are usually better to continue fine-tuning.\n",
    "        upcast_attention (`bool`, *optional*, defaults to `None`):\n",
    "            Whether the attention computation should always be upcasted. This is necessary when running stable\n",
    "            diffusion 2.1.\n",
    "        device (`str`, *optional*, defaults to `None`):\n",
    "            The device to use. Pass `None` to determine automatically. :param from_safetensors: If `checkpoint_path` is\n",
    "            in `safetensors` format, load checkpoint with safetensors instead of PyTorch. :return: A\n",
    "            StableDiffusionInpaintPipeline object representing the passed-in `.ckpt`/`.safetensors` file.\n",
    "    \"\"\"\n",
    "    if prediction_type == \"v-prediction\":\n",
    "        prediction_type = \"v_prediction\"\n",
    "\n",
    "    # if not is_omegaconf_available():\n",
    "    #     raise ValueError(BACKENDS_MAPPING[\"omegaconf\"][1])\n",
    "\n",
    "    from omegaconf import OmegaConf\n",
    "\n",
    "    # if from_safetensors:\n",
    "    #     if not is_safetensors_available():\n",
    "    #         raise ValueError(BACKENDS_MAPPING[\"safetensors\"][1])\n",
    "\n",
    "    #     from safetensors import safe_open\n",
    "\n",
    "    #     checkpoint = {}\n",
    "    #     with safe_open(checkpoint_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    #         for key in f.keys():\n",
    "    #             checkpoint[key] = f.get_tensor(key)\n",
    "    # else:\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Sometimes models don't have the global_step item\n",
    "    if \"global_step\" in checkpoint:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "    else:\n",
    "        print(\"global_step key not found in model\")\n",
    "        global_step = None\n",
    "\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        checkpoint = checkpoint[\"state_dict\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        if original_config_file is None:\n",
    "            key_name = \"model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\"\n",
    "\n",
    "            original_config_file = os.path.join(tmpdir, \"inference.yaml\")\n",
    "            if key_name in checkpoint and checkpoint[key_name].shape[-1] == 1024:\n",
    "                if not os.path.isfile(\"v2-inference-v.yaml\"):\n",
    "                    # model_type = \"v2\"\n",
    "                    r = requests.get(\n",
    "                        \" https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inference-v.yaml\"\n",
    "                    )\n",
    "                    open(original_config_file, \"wb\").write(r.content)\n",
    "\n",
    "                if global_step == 110000:\n",
    "                    # v2.1 needs to upcast attention\n",
    "                    upcast_attention = True\n",
    "            else:\n",
    "                if not os.path.isfile(\"v1-inference.yaml\"):\n",
    "                    # model_type = \"v1\"\n",
    "                    r = requests.get(\n",
    "                        \" https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\"\n",
    "                    )\n",
    "                    open(original_config_file, \"wb\").write(r.content)\n",
    "\n",
    "        original_config = OmegaConf.load(original_config_file)\n",
    "\n",
    "    if num_in_channels is not None:\n",
    "        original_config[\"model\"][\"params\"][\"unet_config\"][\"params\"][\"in_channels\"] = num_in_channels\n",
    "\n",
    "    if (\n",
    "        \"parameterization\" in original_config[\"model\"][\"params\"]\n",
    "        and original_config[\"model\"][\"params\"][\"parameterization\"] == \"v\"\n",
    "    ):\n",
    "        if prediction_type is None:\n",
    "            # NOTE: For stable diffusion 2 base it is recommended to pass `prediction_type==\"epsilon\"`\n",
    "            # as it relies on a brittle global step parameter here\n",
    "            prediction_type = \"epsilon\" if global_step == 875000 else \"v_prediction\"\n",
    "        if image_size is None:\n",
    "            # NOTE: For stable diffusion 2 base one has to pass `image_size==512`\n",
    "            # as it relies on a brittle global step parameter here\n",
    "            image_size = 512 if global_step == 875000 else 768\n",
    "    else:\n",
    "        if prediction_type is None:\n",
    "            prediction_type = \"epsilon\"\n",
    "        if image_size is None:\n",
    "            image_size = 512\n",
    "\n",
    "    num_train_timesteps = original_config.model.params.timesteps\n",
    "    beta_start = original_config.model.params.linear_start\n",
    "    beta_end = original_config.model.params.linear_end\n",
    "\n",
    "    scheduler = DDIMScheduler(\n",
    "        beta_end=beta_end,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        beta_start=beta_start,\n",
    "        num_train_timesteps=num_train_timesteps,\n",
    "        steps_offset=1,\n",
    "        clip_sample=False,\n",
    "        set_alpha_to_one=False,\n",
    "        prediction_type=prediction_type,\n",
    "    )\n",
    "    # make sure scheduler works correctly with DDIM\n",
    "    scheduler.register_to_config(clip_sample=False)\n",
    "\n",
    "    if scheduler_type == \"pndm\":\n",
    "        config = dict(scheduler.config)\n",
    "        config[\"skip_prk_steps\"] = True\n",
    "        scheduler = PNDMScheduler.from_config(config)\n",
    "    elif scheduler_type == \"lms\":\n",
    "        scheduler = LMSDiscreteScheduler.from_config(scheduler.config)\n",
    "    elif scheduler_type == \"heun\":\n",
    "        scheduler = HeunDiscreteScheduler.from_config(scheduler.config)\n",
    "    elif scheduler_type == \"euler\":\n",
    "        scheduler = EulerDiscreteScheduler.from_config(scheduler.config)\n",
    "    elif scheduler_type == \"euler-ancestral\":\n",
    "        scheduler = EulerAncestralDiscreteScheduler.from_config(scheduler.config)\n",
    "    elif scheduler_type == \"dpm\":\n",
    "        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n",
    "    elif scheduler_type == \"ddim\":\n",
    "        scheduler = scheduler\n",
    "    else:\n",
    "        raise ValueError(f\"Scheduler of type {scheduler_type} doesn't exist!\")\n",
    "\n",
    "    # Convert the UNet2DConditionModel model.\n",
    "    unet_config = create_unet_diffusers_config(original_config, image_size=image_size)\n",
    "    unet_config[\"upcast_attention\"] = upcast_attention\n",
    "    unet = UNet2DConditionModel(**unet_config)\n",
    "\n",
    "    converted_unet_checkpoint = convert_ldm_unet_checkpoint(\n",
    "        checkpoint, unet_config, path=checkpoint_path, extract_ema=extract_ema\n",
    "    )\n",
    "\n",
    "    unet.load_state_dict(converted_unet_checkpoint)\n",
    "\n",
    "    # Convert the VAE model.\n",
    "    vae_config = create_vae_diffusers_config(original_config, image_size=image_size)\n",
    "    converted_vae_checkpoint = convert_ldm_vae_checkpoint(checkpoint, vae_config)\n",
    "\n",
    "    vae = AutoencoderKL(**vae_config)\n",
    "    vae.load_state_dict(converted_vae_checkpoint)\n",
    "\n",
    "    # Convert the text model.\n",
    "    if model_type is None:\n",
    "        model_type = original_config.model.params.cond_stage_config.target.split(\".\")[-1]\n",
    "        logger.debug(f\"no `model_type` given, `model_type` inferred as: {model_type}\")\n",
    "\n",
    "    if model_type == \"FrozenOpenCLIPEmbedder\":\n",
    "        text_model = convert_open_clip_checkpoint(checkpoint)\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"tokenizer\")\n",
    "\n",
    "        if stable_unclip is None:\n",
    "            pipe = StableDiffusionInpaintPipeline(\n",
    "                vae=vae,\n",
    "                text_encoder=text_model,\n",
    "                tokenizer=tokenizer,\n",
    "                unet=unet,\n",
    "                scheduler=scheduler,\n",
    "                safety_checker=None,\n",
    "                feature_extractor=None,\n",
    "                requires_safety_checker=False,\n",
    "            )\n",
    "        else:\n",
    "            image_normalizer, image_noising_scheduler = stable_unclip_image_noising_components(\n",
    "                original_config, clip_stats_path=clip_stats_path, device=device\n",
    "            )\n",
    "\n",
    "            if stable_unclip == \"img2img\":\n",
    "                feature_extractor, image_encoder = stable_unclip_image_encoder(original_config)\n",
    "\n",
    "                pipe = StableUnCLIPImg2ImgPipeline(\n",
    "                    # image encoding components\n",
    "                    feature_extractor=feature_extractor,\n",
    "                    image_encoder=image_encoder,\n",
    "                    # image noising components\n",
    "                    image_normalizer=image_normalizer,\n",
    "                    image_noising_scheduler=image_noising_scheduler,\n",
    "                    # regular denoising components\n",
    "                    tokenizer=tokenizer,\n",
    "                    text_encoder=text_model,\n",
    "                    unet=unet,\n",
    "                    scheduler=scheduler,\n",
    "                    # vae\n",
    "                    vae=vae,\n",
    "                )\n",
    "            elif stable_unclip == \"txt2img\":\n",
    "                if stable_unclip_prior is None or stable_unclip_prior == \"karlo\":\n",
    "                    karlo_model = \"kakaobrain/karlo-v1-alpha\"\n",
    "                    prior = PriorTransformer.from_pretrained(karlo_model, subfolder=\"prior\")\n",
    "\n",
    "                    prior_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "                    prior_text_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "                    prior_scheduler = UnCLIPScheduler.from_pretrained(karlo_model, subfolder=\"prior_scheduler\")\n",
    "                    prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"unknown prior for stable unclip model: {stable_unclip_prior}\")\n",
    "\n",
    "                pipe = StableUnCLIPPipeline(\n",
    "                    # prior components\n",
    "                    prior_tokenizer=prior_tokenizer,\n",
    "                    prior_text_encoder=prior_text_model,\n",
    "                    prior=prior,\n",
    "                    prior_scheduler=prior_scheduler,\n",
    "                    # image noising components\n",
    "                    image_normalizer=image_normalizer,\n",
    "                    image_noising_scheduler=image_noising_scheduler,\n",
    "                    # regular denoising components\n",
    "                    tokenizer=tokenizer,\n",
    "                    text_encoder=text_model,\n",
    "                    unet=unet,\n",
    "                    scheduler=scheduler,\n",
    "                    # vae\n",
    "                    vae=vae,\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"unknown `stable_unclip` type: {stable_unclip}\")\n",
    "    elif model_type == \"PaintByExample\":\n",
    "        vision_model = convert_paint_by_example_checkpoint(checkpoint)\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        pipe = PaintByExamplePipeline(\n",
    "            vae=vae,\n",
    "            image_encoder=vision_model,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "    elif model_type == \"FrozenCLIPEmbedder\":\n",
    "        text_model = convert_ldm_clip_checkpoint(checkpoint)\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        pipe = StableDiffusionInpaintPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_model,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "    else:\n",
    "        text_config = create_ldm_bert_config(original_config)\n",
    "        text_model = convert_ldm_bert_checkpoint(checkpoint, text_config)\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "        pipe = LDMTextToImagePipeline(vqvae=vae, bert=text_model, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step key not found in model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/data3/model_checkpoints/DIFFUSION_DB/Diffusion_models/SD-v1-5-ckpts/real-korean-v15-step5-inpainting.ckpt'\n",
    "original_config_file = '/data3/model_checkpoints/DIFFUSION_DB/Diffusion_models/configs/v1-inpainting-inference.yaml'\n",
    "dump_path = '/data3/model_checkpoints/DIFFUSION_DB/Diffusion_models/diffusers/v15/real-korean-v15-step5-inpainting'\n",
    "\n",
    "\n",
    "pipe = load_pipeline_inpainting_stable_diffusion_ckpt(\n",
    "    checkpoint_path = checkpoint_path,\n",
    "    original_config_file = original_config_file\n",
    ")\n",
    "\n",
    "pipe.save_pretrained(dump_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
